{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b318982f",
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: replacing module Flyonic.\n",
      "WARNING: using Flyonic.eth_vtol_param in module Main conflicts with an existing identifier.\n",
      "\u001b[36m\u001b[1m┌ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mMeshCat server started. You can open the visualizer by visiting the following URL in your browser:\n",
      "\u001b[36m\u001b[1m└ \u001b[22m\u001b[39mhttp://127.0.0.1:8707\n",
      "WARNING: redefinition of constant INITIAL_POSITION. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant INITIAL_ROTATION. This may fail, cause incorrect answers, or produce other errors.\n",
      "WARNING: redefinition of constant SAMPLE_WAYPOINTS. This may fail, cause incorrect answers, or produce other errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mTest Summary:              | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "random policy with VtolEnv | \u001b[32m2000  \u001b[39m\u001b[36m 2000  \u001b[39m\u001b[0m0.0s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   2%|█                                        |  ETA: 0:03:39\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 10000: -1002.4891306545134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   4%|██                                       |  ETA: 0:03:14\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 20000: -615.9665850890221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   6%|███                                      |  ETA: 0:03:38\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 30000: -194.72378502942166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:   8%|████                                     |  ETA: 0:04:07\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 40000: -827.4621232799346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  10%|████                                     |  ETA: 0:04:23\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 50000: -369.186506011092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  12%|█████                                    |  ETA: 0:04:32\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 60000: -252.75424595344444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  14%|██████                                   |  ETA: 0:04:41\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 70000: -156.75168072463268\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  16%|███████                                  |  ETA: 0:04:21\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 80000: -207.98308036107784\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  18%|████████                                 |  ETA: 0:04:21\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 90000: -156.34906939744727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  20%|█████████                                |  ETA: 0:04:11\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 100000 saved to ./RL_models_leo/vtol_2D_ppo_100000.bson\n",
      "test reward at step 100000: -187.43541365042253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  22%|█████████                                |  ETA: 0:04:08\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 110000: -238.73993280116653\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  24%|██████████                               |  ETA: 0:04:05\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 120000: -541.4971441309202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  26%|███████████                              |  ETA: 0:04:11\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 130000: -171.48983392504655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  28%|████████████                             |  ETA: 0:04:07\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 140000: -176.92835082793448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  29%|█████████████                            |  ETA: 0:04:03\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 150000: -167.4883562409846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  32%|██████████████                           |  ETA: 0:04:07\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 160000: -304.6135674989451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  33%|██████████████                           |  ETA: 0:03:55\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 170000: -215.51033475669502\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  36%|███████████████                          |  ETA: 0:03:51\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 180000: -401.35339866985566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  38%|████████████████                         |  ETA: 0:03:48\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 190000: -196.64914698893458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  40%|█████████████████                        |  ETA: 0:03:44\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 200000 saved to ./RL_models_leo/vtol_2D_ppo_200000.bson\n",
      "test reward at step 200000: -196.6288923575555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  42%|██████████████████                       |  ETA: 0:03:39\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 210000: -218.75446906128715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  44%|███████████████████                      |  ETA: 0:03:33\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 220000: -540.970159375098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  46%|███████████████████                      |  ETA: 0:03:32\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 230000: -636.9562060068815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  48%|████████████████████                     |  ETA: 0:03:34\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 240000: -270.65942816182064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  50%|█████████████████████                    |  ETA: 0:03:34\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 250000: -596.2532917201647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  52%|██████████████████████                   |  ETA: 0:03:32\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 260000: -219.4826522581006\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  54%|███████████████████████                  |  ETA: 0:03:30\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 270000: -243.15912801051542\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  56%|███████████████████████                  |  ETA: 0:03:28\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 280000: -352.1160519659258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  58%|████████████████████████                 |  ETA: 0:03:31\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 290000: -206.94636057548564\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  60%|█████████████████████████                |  ETA: 0:03:24\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 300000 saved to ./RL_models_leo/vtol_2D_ppo_300000.bson\n",
      "test reward at step 300000: -222.5576817733031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  62%|██████████████████████████               |  ETA: 0:03:26\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 310000: -210.17694557205058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  64%|███████████████████████████              |  ETA: 0:03:18\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 320000: -185.06286190489843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  66%|███████████████████████████              |  ETA: 0:03:06\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 330000: -346.9274536229429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  68%|████████████████████████████             |  ETA: 0:03:01\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 340000: -164.91830873280492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  70%|█████████████████████████████            |  ETA: 0:02:56\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 350000: -296.35216580026554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  72%|██████████████████████████████           |  ETA: 0:02:48\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 360000: -172.99496676079818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  74%|███████████████████████████████          |  ETA: 0:02:40\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 370000: -189.75405181355356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  75%|███████████████████████████████          |  ETA: 0:02:37\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 380000: -294.69799417918756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  77%|████████████████████████████████         |  ETA: 0:02:24\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 390000: -154.76663624422915\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  80%|█████████████████████████████████        |  ETA: 0:02:14\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 400000 saved to ./RL_models_leo/vtol_2D_ppo_400000.bson\n",
      "test reward at step 400000: -160.49693926024617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  82%|██████████████████████████████████       |  ETA: 0:02:03\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 410000: -125.33972039592466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  84%|███████████████████████████████████      |  ETA: 0:01:50\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 420000: -90.73047118010342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  85%|████████████████████████████████████     |  ETA: 0:01:42\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 430000: -83.3767870543881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  88%|████████████████████████████████████     |  ETA: 0:01:29\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 440000: -89.89468462820747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  90%|█████████████████████████████████████    |  ETA: 0:01:15\u001b[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 450000: -41.413345832150775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress:  92%|██████████████████████████████████████   |  ETA: 0:01:00\u001b[39m"
     ]
    }
   ],
   "source": [
    "## Init Bionic VTOL\n",
    "include(\"../Flyonic.jl\");\n",
    "using .Flyonic;\n",
    "\n",
    "using Rotations; # used for initial position\n",
    "\n",
    "using ReinforcementLearning;\n",
    "using StableRNGs;\n",
    "using Flux;\n",
    "using Flux.Losses;\n",
    "using Random;\n",
    "using IntervalSets;\n",
    "using LinearAlgebra;\n",
    "using Distributions;\n",
    "\n",
    "using Plots;\n",
    "using Statistics;\n",
    "\n",
    "using BSON: @save, @load # save mode\n",
    "create_visualization();\n",
    "\n",
    "# indicates how many threads Julia was started with. This is important for the multi-threaded environment\n",
    "Threads.nthreads()\n",
    "### Create Reinforcement Learning Environment\n",
    "\n",
    "##### first coordinate - red axis - x\n",
    "##### second coordinate - green axis - y\n",
    "##### third coordinate - blue axis - z\n",
    "\n",
    "\n",
    "const NUM_WAYPOINTS = 2;\n",
    "const INITIAL_POSITION = [0.0, 0.0, 0.0];\n",
    "const INITIAL_ROTATION = [1.0 0.0 0.0; 0.0 1.0 0.0; 0.0 0.0 1.0];\n",
    "const SAMPLE_WAYPOINTS = [[0.0, 0.0, 0.0], [-10.0, 0.0, 5.0]];\n",
    "\n",
    "w_max = 25.0; #maximum rotational speed before getting punished\n",
    "timeout = 15.0; # simulation end time \n",
    "floor_level = -10.0; #as in aviation\n",
    "v_expected = 3.0; #maximum speed before getting punished\n",
    "\n",
    "######################################################################\n",
    "mutable struct VtolEnv{A,T,ACT,R<:AbstractRNG} <: AbstractEnv # Parametric Constructor for a subtype of AbstractEnv\n",
    "    action_space::A # action space\n",
    "    observation_space::Space{Vector{ClosedInterval{T}}} # observation space\n",
    "    state::Vector{T} # current state space\n",
    "    action::ACT # action space\n",
    "    done::Bool # done\n",
    "    t::T # time\n",
    "    rng::R # random number generator\n",
    "\n",
    "    name::String # for multible environments\n",
    "    visualization::Bool # visualization\n",
    "    realtime::Bool # realtime\n",
    "\n",
    "    x_previous::Vector{T} # previous position\n",
    "    x_W::Vector{T} # current position\n",
    "    v_B::Vector{T} # velocity\n",
    "    R_W::Matrix{T} # current rotation\n",
    "    w_B::Vector{T} # rotation velocitiy\n",
    "    wind_W::Vector{T} # wind\n",
    "    delta_t::T # simulation time step\n",
    "\n",
    "    ################################ TODO ################################\n",
    "    # Extend the environment here.\n",
    "    # Everything you need additionaly in your environment also go in here.\n",
    "    # E.g. a trajectory\n",
    "\n",
    "    waypoints::Vector{Vector{T}} # waypoints\n",
    "    proximity_tolerance::T # proximity tolerance\n",
    "    v_min::T # minimum required velocity\n",
    "    v_max::T # maximum allowed velocity\n",
    "\n",
    "    ######################################################################\n",
    "end\n",
    "\n",
    "\n",
    "# define a keyword-based constructor for the type declared in the mutable struct typedef.\n",
    "# It could also be done with the macro Base.@kwdef.\n",
    "function VtolEnv(;\n",
    "    rng = Random.GLOBAL_RNG, # random number generation\n",
    "    name = \"vtol\",\n",
    "            visualization = false,\n",
    "    realtime = false,\n",
    "    kwargs...) # let the function take an arbitrary number of keyword arguments\n",
    "\n",
    "    T = Float64; # explicit type which is used e.g. in state. Cannot be altered due to the poor matrix defininon.\n",
    "    A = Space{Vector{ClosedInterval{T}}};\n",
    "\n",
    "    action_space = Space(\n",
    "        ClosedInterval{T}[\n",
    "            0.0..2.0, # propeller 1\n",
    "            0.0..2.0, # propeller 2\n",
    "            ],\n",
    "    ) # propeller 1 and 2\n",
    "\n",
    "    state_space = Space( # Three continuous values in state space.\n",
    "        ClosedInterval{T}[\n",
    "            ################################ TODO ################################\n",
    "            # Implement an observation space.\n",
    "            # Here is an example space. You can change it if desired.\n",
    "            # You have to extend it.\n",
    "            # Orientate yourself on the observation space from the paper.\n",
    "\n",
    "            typemin(T)..typemax(T), # previous position along x WORLD coordinates\n",
    "            typemin(T)..typemax(T), # previous position along z WORLD coordinates\n",
    "\n",
    "            typemin(T)..typemax(T), # current position along x WORLD coordinates\n",
    "            typemin(T)..typemax(T), # current position along z WORLD coordinates\n",
    "\n",
    "            typemin(T)..typemax(T), # orientation along x WORLD coordinates\n",
    "            typemin(T)..typemax(T), # orientation along z WORLD coordinates\n",
    "\n",
    "            typemin(T)..typemax(T), # velocity along x BODY coordinates\n",
    "            typemin(T)..typemax(T), # velocity along y BODY coordinates\n",
    "\n",
    "            typemin(T)..typemax(T), # rotational velocity along z BODY coordinates\n",
    "\n",
    "            #changed back to world for calculating guiding path easier\n",
    "            typemin(T)..typemax(T), # position of target along x WORLD coordinates\n",
    "            typemin(T)..typemax(T), # position of target along z WORLD coordinates\n",
    "\n",
    "            #changed so the drone knows the guiding path \n",
    "            typemin(T)..typemax(T), # position of previous WP along x WORLD coordinates\n",
    "            typemin(T)..typemax(T), # position of previous WP along z WORLD coordinates\n",
    "\n",
    "            ######################################################################\n",
    "            ],\n",
    "    )\n",
    "\n",
    "    if visualization #visualizes VTOL\n",
    "        create_VTOL(name, actuators = true, color_vec=[1.0; 1.0; 0.6; 1.0]);\n",
    "    end\n",
    "\n",
    "    environment = VtolEnv(\n",
    "        action_space, # action space\n",
    "        state_space, # observation space\n",
    "        zeros(T, length(state_space)), # current state space\n",
    "        rand(action_space), # initialization action\n",
    "        false, # episode done\n",
    "        0.0, # time\n",
    "        rng, # random number generator\n",
    "\n",
    "        name,\n",
    "        visualization,\n",
    "        realtime,\n",
    "\n",
    "        zeros(T, 3), # x_previous, previous position\n",
    "        zeros(T, 3), # x_W, current position\n",
    "        zeros(T, 3), # v_B, velocity\n",
    "        [1.0 0.0 0.0; 0.0 1.0 0.0; 0.0 0.0 1.0], # R_W, current rotation\n",
    "        zeros(T, 3), # w_B\n",
    "        zeros(T, 3), # wind_W\n",
    "        T(0.02), # simulation time step\n",
    "\n",
    "        ################################## TODO ##################################\n",
    "        # Initialization everything you need additionaly in your environment here\n",
    "        SAMPLE_WAYPOINTS,\n",
    "        1e-5, # proximity tolerance\n",
    "        1.0, # minimum required velocity\n",
    "        20.0, # maximum allowed velocity\n",
    "        ##########################################################################\n",
    "    )\n",
    "\n",
    "    reset!(environment)\n",
    "    return environment\n",
    "end;\n",
    "\n",
    "methods(VtolEnv)\n",
    "\n",
    "\n",
    "# Just for explanation:\n",
    "# 1. A mutable Struct is created. A struct is a constructor and a constructor is a function that creates new objects.\n",
    "# 2. A outer keyword-based constructor method is added for the type declared in the mutable struct typedef before.'\n",
    "# So now we have a function with two methods. Julia will decide which method to call by multiple dispatch.\n",
    "\n",
    "## Define the RL interface\n",
    "\n",
    "Random.seed!(env::VtolEnv, seed) = Random.seed!(env.rng, seed)\n",
    "RLBase.action_space(env::VtolEnv) = env.action_space\n",
    "RLBase.state_space(env::VtolEnv) = env.observation_space\n",
    "RLBase.is_terminated(env::VtolEnv) = env.done\n",
    "RLBase.state(env::VtolEnv) = env.state\n",
    "\n",
    "function computeReward(env::VtolEnv{A,T}) where {A,T}\n",
    "    reward = 0.0\n",
    "\n",
    "    ################################ TODO ################################\n",
    "    # Implement the reward function.\n",
    "    # Orientate on the paper.\n",
    "\n",
    "    # Hyperparameters\n",
    "    kp = 5.0;\n",
    "    kw = 0.5; \n",
    "    kwp = 25.0;\n",
    "    kv = 1.0;\n",
    "    kb = 25.0; # changed - added (see 'bad' below) #Stop perverse incentives\n",
    "\n",
    "    # Gates and their length\n",
    "    gates = [[0.0, 0.0, 0.0], [env.state[10], 0.0, env.state[11]]];\n",
    "    n = length(gates);\n",
    "\n",
    "    # closest point on the guiding path phi\n",
    "    # its line-segment index lp\n",
    "    x_old = [env.state[1], 0.0, env.state[2]];\n",
    "    x_new = [env.state[3], 0.0, env.state[4]];\n",
    "    lp_old, phi_old = calculate_progress(gates, x_old);\n",
    "    lp_new, phi_new = calculate_progress(gates, x_new);\n",
    "\n",
    "    # previous time step spt_old\n",
    "    # current time step spt_new\n",
    "    spt_old = 0.0;\n",
    "    spt_new = 0.0;\n",
    "    for i in 1:(lp_old-1)\n",
    "        spt_old += (norm(gates[i+1] - gates[i]) + norm(phi_old - gates[lp_old]));\n",
    "    end\n",
    "    for i in 1:(lp_new-1)\n",
    "        spt_new += (norm(gates[i+1] - gates[i]) + norm(phi_new - gates[lp_old]));\n",
    "    end\n",
    "\n",
    "    # rogress reward rpt at time t is as a difference in reached\n",
    "    # distance between the current and previous time step\n",
    "    rpt = spt_new - spt_old\n",
    "    spt = spt_new\n",
    "\n",
    "    # The sum that is later goig to be the divisor\n",
    "    # for the reached distance reward\n",
    "    divisor_g = 0.0;\n",
    "    for i in 1:(n-1)\n",
    "        divisor_g += norm(gates[i+1] - gates[i]);\n",
    "    end\n",
    "\n",
    "    # reached distance reward ks\n",
    "    ks = 2 * env.v_max * env.delta_t / divisor_g;\n",
    "\n",
    "    # waypoint index wpi must be same as the index of the current line segment\n",
    "    wpi = lp_old+1; #test\n",
    "    # distance to new waypoint dw\n",
    "    dwp = norm(env.x_W - gates[wpi]);\n",
    "    # tolerance for proximity to a waypoint\n",
    "    r_tol = env.proximity_tolerance;\n",
    "\n",
    "    # waypoint reward rwp\n",
    "    rwp = exp(-dwp/r_tol);\n",
    "\n",
    "    # no obstacles\n",
    "    collision = false;\n",
    "    # terminal reward rt\n",
    "    rt = collision ? -10 : 0;\n",
    "\n",
    "    # absolute velocity\n",
    "    v_vector = [env.state[7], env.state[8], 0.0]\n",
    "    v = norm(v_vector);\n",
    "    # rotation speed\n",
    "    w = abs(env.state[9]);\n",
    "    # distance from closes point on the guiding path\n",
    "    gd = norm(x_new - phi_new);\n",
    "    # max distance\n",
    "    dmax = 0.3;\n",
    "\n",
    "    # punishment for flying too damn fast\n",
    "    rv = max(0, v - v_expected); #changed not to punish slow flight\n",
    "\n",
    "    # Scaling factors\n",
    "    svmax = v > env.v_max ? 10^(env.v_max - v) : 1.0;\n",
    "    svmin = v < env.v_min ?  10^(v - env.v_min) : 1.0;\n",
    "    sgd = gd > dmax  ? exp(dmax - gd) : 1.0;\n",
    "\n",
    "    #changed - added punishment for intentionally stopping the simulation\n",
    "    bad = ( max(0, w + 1 - w_max, 0.2* (1 + floor_level - env.x_W[3]) ) ) * (timeout - env.t) #^2 #hitting the floor maybe?\n",
    "\n",
    "    # Ultimate scaling factor\n",
    "    s = svmax * svmin * sgd;\n",
    "\n",
    "    # Scaling the rewards\n",
    "    kp = s * kp;\n",
    "    ks = s * ks;\n",
    "\n",
    "    reward = kp * rpt + ks * spt + kwp * rwp + rt - kw * w - kv * rv - kb * bad;\n",
    "\n",
    "    ################################################################################################\n",
    "\n",
    "    return reward\n",
    "end\n",
    "\n",
    "RLBase.reward(env::VtolEnv{A,T}) where {A,T} = computeReward(env)\n",
    "\n",
    "function RLBase.reset!(env::VtolEnv{A,T}) where {A,T}\n",
    "    # Visualize initial state\n",
    "    if env.visualization\n",
    "        set_transform(env.name, env.x_W, QuatRotation(env.R_W));\n",
    "        set_actuators(env.name, [0.0; 0.0; 0.0; 0.0]);\n",
    "    end\n",
    "\n",
    "    env.x_W = [0.0; 0.0; 0.0];\n",
    "    env.v_B = [0.0; 0.0; 0.0];\n",
    "    env.R_W = Matrix(UnitQuaternion(RotZ(-pi/2.0) * RotY(-pi/2.0) * RotX(pi)));\n",
    "\n",
    "    env.w_B = [0.0; 0.0; 0.0];\n",
    "    env.wind_W = [0.0; 0.0; 0.0];\n",
    "\n",
    "    env.t = 0.0;\n",
    "    env.action = [0.0, 0.0];\n",
    "    env.done = false;\n",
    "\n",
    "    ################################ TODO ################################\n",
    "    # Reset environment.\n",
    "    # Is called if the training terminates\n",
    "    # (e.g. if drone crashes or successfully reaches point)\n",
    "    # HINT: Everything you added to your environment needs to be reseted.\n",
    "    #       Compare it with the initialization.\n",
    "\n",
    "    env.x_previous = [0.0; 0.0; 0.0]; # starting position\n",
    "    env.delta_t = T(0.02); # Δ time\n",
    "\n",
    "    env.waypoints = SAMPLE_WAYPOINTS;\n",
    "    env.proximity_tolerance = 1e-1;\n",
    "\n",
    "\n",
    "    # Visualize the waypoints\n",
    "    radius = 0.1;\n",
    "    visualize_waypoints(env.waypoints, radius);\n",
    "    ######################################################################\n",
    "\n",
    "    nothing\n",
    "end;\n",
    "\n",
    "# defines a methods for a callable object.\n",
    "# So when a VtolEnv object is created, it has this method that can be called\n",
    "function (env::VtolEnv)(a)\n",
    "    # set the propeller trust and the two flaps 2D case\n",
    "    # flaps set to 0.0\n",
    "    next_action = [a[1], a[2], 0.0, 0.0]\n",
    "\n",
    "    _step!(env, next_action)\n",
    "end\n",
    "\n",
    "env = VtolEnv();\n",
    "methods(env) # Just to explain which methods the object has\n",
    "\n",
    "\n",
    "\n",
    "function _step!(env::VtolEnv, next_action)\n",
    "\n",
    "\n",
    "    env.x_previous = env.x_W;\n",
    "\n",
    "    # caluclate wind impact\n",
    "    v_in_wind_B = vtol_add_wind(env.v_B, env.R_W, env.wind_W);\n",
    "\n",
    "    # caluclate aerodynamic forces\n",
    "    torque_B, force_B = vtol_model(v_in_wind_B, next_action, eth_vtol_param);\n",
    "\n",
    "    # Limit to 2D\n",
    "    force_B[3] = 0.0; # Body Z\n",
    "    env.v_B[3] = 0.0;\n",
    "    torque_B[1] = 0.0;\n",
    "    torque_B[2] = 0.0;  # Body X and Y\n",
    "    env.w_B[1] = 0.0;\n",
    "    env.w_B[2] = 0.0;\n",
    "\n",
    "    # integrate rigid body dynamics for delta_t\n",
    "    env.x_W, env.v_B, env.R_W, env.w_B, time, trafo = rigid_body_simple(torque_B, force_B, env.x_W, env.v_B, env.R_W, env.w_B, env.t, env.delta_t, eth_vtol_param);\n",
    "\n",
    "    if env.realtime\n",
    "        sleep(env.delta_t); # just a dirty hack. this is of course slower than real time.\n",
    "    end\n",
    "\n",
    "    # Visualize the new state\n",
    "    if env.visualization\n",
    "        set_transform(env.name, env.x_W, QuatRotation(env.R_W));\n",
    "        set_actuators(env.name, next_action);\n",
    "    end\n",
    "\n",
    "    env.t += env.delta_t\n",
    "\n",
    "    #target = trafo * (env.waypoints[2] - env.x_W)\n",
    "\n",
    "\n",
    "    env.state[3] = env.x_W[1]; # position along x\n",
    "    env.state[4] = env.x_W[3]; # position along z\n",
    "\n",
    "    env.state[5] = env.R_W[1,1]; # orientation along x\n",
    "    env.state[6] = env.R_W[3,1]; # orientation along z\n",
    "\n",
    "    env.state[7] = env.v_B[1]; # velocity along x BODY coordinates\n",
    "    env.state[8] = env.v_B[2]; # velocity along y BODY coordinates\n",
    "\n",
    "    env.state[9] = env.w_B[3];  # rotational velocity along z BODY coordinates\n",
    "\n",
    "    ################################ TODO ################################\n",
    "\n",
    "    env.state[1] = env.x_previous[1]; # update previous x-coordinate\n",
    "    env.state[2] = env.x_previous[3]; # update previous z-coordinate\n",
    "\n",
    "    env.state[10] = env.waypoints[2][1] # position of target along x WORLD coordinates\n",
    "    env.state[11] = env.waypoints[2][3] # position of target along z WORLD coordinates\n",
    "    \n",
    "    env.state[12] = env.waypoints[1][1] # position of target along x WORLD coordinates\n",
    "    env.state[13] = env.waypoints[1][3] # position of target along z WORLD coordinates\n",
    "    #env.state[10] = target[1] # position of target along x BODY coordinates\n",
    "    #env.state[11] = target[3] # position of target along z BODY coordinates\n",
    "\n",
    "\n",
    "    # maximum rotation speed\n",
    "\n",
    "    env.done =\n",
    "        norm(env.w_B) > w_max || # stop if body rate is too high\n",
    "        norm(env.v_B) > env.v_max || # stop if body is too fast\n",
    "        env.state[4] < floor_level ||\n",
    "        env.t > timeout ||\n",
    "        norm(env.waypoints[2] - env.x_W) < env.proximity_tolerance  # target reached\n",
    "    ######################################################################\n",
    "\n",
    "    nothing\n",
    "end;\n",
    "\n",
    "RLBase.test_runnable!(env)\n",
    "\n",
    "# changed to 10s (5s before) per point and 5.0m too far off path (2.0 before)\n",
    "# Show an overview of the environment.\n",
    "\n",
    "## Setup of a reinforcement learning experiment.\n",
    "\n",
    "seed = 123\n",
    "rng = StableRNG(seed)\n",
    "    N_ENV = 8\n",
    "    UPDATE_FREQ = 1024\n",
    "\n",
    "    vtol_envs = [\n",
    "        # use different names for the visualization\n",
    "        VtolEnv(; rng = StableRNG(hash(seed+i)), name = \"vtol$i\") for i in 1:N_ENV\n",
    "    ];\n",
    "    # define multiple environments for parallel training\n",
    "    env = MultiThreadEnv(vtol_envs)\n",
    "\n",
    "    # Define the function approximator\n",
    "    # (optional) TODO: change architecture\n",
    "    # TODO: research briefly what Actor Critic is\n",
    "    # (optional) TODO: change optimizer\n",
    "    # TODO: research what ADAM is\n",
    "    ns, na = length(state(env[1])), length(action_space(env[1]))\n",
    "    approximator = ActorCritic(\n",
    "                #ns - number states as input\n",
    "                #3 layer; last layer splitted in mean and variance; then action is sampled\n",
    "                actor = GaussianNetwork(\n",
    "                    pre = Chain(\n",
    "                    Dense(ns, 16, relu; initW = glorot_uniform(rng)),#\n",
    "                    Dense(16, 16, relu; initW = glorot_uniform(rng)),\n",
    "                    ),\n",
    "                    μ = Chain(Dense(16, na; initW = glorot_uniform(rng))),\n",
    "                    logσ = Chain(Dense(16, na; initW = glorot_uniform(rng))),\n",
    "                ),\n",
    "                critic = Chain(\n",
    "                    Dense(ns, 16, relu; initW = glorot_uniform(rng)),\n",
    "                    Dense(16, 16, relu; initW = glorot_uniform(rng)),\n",
    "                    Dense(16, 1; initW = glorot_uniform(rng)),\n",
    "                ),\n",
    "                optimizer = ADAM(1e-3),\n",
    "            );\n",
    "\n",
    "        agent = Agent( # A wrapper of an AbstractPolicy\n",
    "        # AbstractPolicy: the policy to use\n",
    "        # (optional) TODO: change eventually\n",
    "        # TODO: research briefly what PPO is\n",
    "        policy = PPOPolicy(;\n",
    "                    approximator = approximator |> gpu,\n",
    "                    update_freq=UPDATE_FREQ,\n",
    "                    dist = Normal,\n",
    "                    # For parameters visit the docu: https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.PPOPolicy\n",
    "                    ),\n",
    "\n",
    "        # AbstractTrajectory: used to store transitions between an agent and an environment source\n",
    "        trajectory = PPOTrajectory(;\n",
    "            capacity = UPDATE_FREQ,\n",
    "            state = Matrix{Float64} => (ns, N_ENV),\n",
    "            action = Matrix{Float64} => (na, N_ENV),\n",
    "            action_log_prob = Vector{Float64} => (N_ENV,),\n",
    "            reward = Vector{Float64} => (N_ENV,),\n",
    "            terminal = Vector{Bool} => (N_ENV,),\n",
    "        ),\n",
    "    );\n",
    "\n",
    "\n",
    "function saveModel(t, agent, env)\n",
    "    model = cpu(agent.policy.approximator)\n",
    "    f = joinpath(\"./RL_models_leo/\", \"vtol_2D_ppo_$t.bson\") # TODO: save model here\n",
    "    @save f model\n",
    "    println(\"parameters at step $t saved to $f\")\n",
    "end;\n",
    "\n",
    "function loadModel()\n",
    "    f = joinpath(\"./RL_models_leo/\", \"vtol_2D_ppo_1500000.bson\") # TODO: load model here\n",
    "    @load f model\n",
    "    return model\n",
    "end;\n",
    "\n",
    "function validate_policy(t, agent, env)\n",
    "    run(agent.policy, test_env, StopAfterEpisode(1), episode_test_reward_hook)\n",
    "    # the result of the hook\n",
    "    println(\"test reward at step $t: $(episode_test_reward_hook.rewards[end])\")\n",
    "\n",
    "end;\n",
    "\n",
    "episode_test_reward_hook = TotalRewardPerEpisode(;is_display_on_exit=false)\n",
    "# create a env only for reward test\n",
    "test_env = VtolEnv(;name = \"testVTOL\", visualization = true, realtime = true);\n",
    "\n",
    "ReinforcementLearning.run(\n",
    "    agent,\n",
    "    env,\n",
    "    StopAfterStep(200_000),\n",
    "    ComposedHook(\n",
    "        DoEveryNStep(saveModel, n=100_000),\n",
    "        DoEveryNStep(validate_policy, n=10_000)),\n",
    ")\n",
    "\n",
    "#agent.policy.approximator = loadModel();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779321f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"600\" height=\"400\" viewBox=\"0 0 2400 1600\">\n<defs>\n  <clipPath id=\"clip740\">\n    <rect x=\"0\" y=\"0\" width=\"2400\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip740)\" d=\"\nM0 1600 L2400 1600 L2400 0 L0 0  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip741\">\n    <rect x=\"480\" y=\"0\" width=\"1681\" height=\"1600\"/>\n  </clipPath>\n</defs>\n<path clip-path=\"url(#clip740)\" d=\"\nM239.19 1486.45 L2352.76 1486.45 L2352.76 47.2441 L239.19 47.2441  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<defs>\n  <clipPath id=\"clip742\">\n    <rect x=\"239\" y=\"47\" width=\"2115\" height=\"1440\"/>\n  </clipPath>\n</defs>\n<polyline clip-path=\"url(#clip742)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  718.783,1486.45 718.783,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip742)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1243.5,1486.45 1243.5,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip742)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  1768.22,1486.45 1768.22,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip742)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  2292.94,1486.45 2292.94,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  239.19,1486.45 2352.76,1486.45 \n  \"/>\n<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  718.783,1486.45 718.783,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1243.5,1486.45 1243.5,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  1768.22,1486.45 1768.22,1467.55 \n  \"/>\n<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  2292.94,1486.45 2292.94,1467.55 \n  \"/>\n<path clip-path=\"url(#clip740)\" d=\"M709.061 1514.29 L727.417 1514.29 L727.417 1518.22 L713.343 1518.22 L713.343 1526.7 Q714.362 1526.35 715.38 1526.19 Q716.399 1526 717.417 1526 Q723.204 1526 726.584 1529.17 Q729.963 1532.34 729.963 1537.76 Q729.963 1543.34 726.491 1546.44 Q723.019 1549.52 716.7 1549.52 Q714.524 1549.52 712.255 1549.15 Q710.01 1548.78 707.602 1548.04 L707.602 1543.34 Q709.686 1544.47 711.908 1545.03 Q714.13 1545.58 716.607 1545.58 Q720.612 1545.58 722.95 1543.48 Q725.288 1541.37 725.288 1537.76 Q725.288 1534.15 722.95 1532.04 Q720.612 1529.94 716.607 1529.94 Q714.732 1529.94 712.857 1530.35 Q711.005 1530.77 709.061 1531.65 L709.061 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M1218.19 1544.91 L1225.83 1544.91 L1225.83 1518.55 L1217.52 1520.21 L1217.52 1515.95 L1225.78 1514.29 L1230.46 1514.29 L1230.46 1544.91 L1238.1 1544.91 L1238.1 1548.85 L1218.19 1548.85 L1218.19 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M1257.54 1517.37 Q1253.93 1517.37 1252.1 1520.93 Q1250.3 1524.47 1250.3 1531.6 Q1250.3 1538.71 1252.1 1542.27 Q1253.93 1545.82 1257.54 1545.82 Q1261.17 1545.82 1262.98 1542.27 Q1264.81 1538.71 1264.81 1531.6 Q1264.81 1524.47 1262.98 1520.93 Q1261.17 1517.37 1257.54 1517.37 M1257.54 1513.66 Q1263.35 1513.66 1266.41 1518.27 Q1269.48 1522.85 1269.48 1531.6 Q1269.48 1540.33 1266.41 1544.94 Q1263.35 1549.52 1257.54 1549.52 Q1251.73 1549.52 1248.65 1544.94 Q1245.6 1540.33 1245.6 1531.6 Q1245.6 1522.85 1248.65 1518.27 Q1251.73 1513.66 1257.54 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M1743.4 1544.91 L1751.04 1544.91 L1751.04 1518.55 L1742.73 1520.21 L1742.73 1515.95 L1751 1514.29 L1755.67 1514.29 L1755.67 1544.91 L1763.31 1544.91 L1763.31 1548.85 L1743.4 1548.85 L1743.4 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M1772.8 1514.29 L1791.16 1514.29 L1791.16 1518.22 L1777.09 1518.22 L1777.09 1526.7 Q1778.1 1526.35 1779.12 1526.19 Q1780.14 1526 1781.16 1526 Q1786.95 1526 1790.33 1529.17 Q1793.71 1532.34 1793.71 1537.76 Q1793.71 1543.34 1790.23 1546.44 Q1786.76 1549.52 1780.44 1549.52 Q1778.27 1549.52 1776 1549.15 Q1773.75 1548.78 1771.34 1548.04 L1771.34 1543.34 Q1773.43 1544.47 1775.65 1545.03 Q1777.87 1545.58 1780.35 1545.58 Q1784.35 1545.58 1786.69 1543.48 Q1789.03 1541.37 1789.03 1537.76 Q1789.03 1534.15 1786.69 1532.04 Q1784.35 1529.94 1780.35 1529.94 Q1778.47 1529.94 1776.6 1530.35 Q1774.75 1530.77 1772.8 1531.65 L1772.8 1514.29 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M2271.71 1544.91 L2288.03 1544.91 L2288.03 1548.85 L2266.09 1548.85 L2266.09 1544.91 Q2268.75 1542.16 2273.33 1537.53 Q2277.94 1532.88 2279.12 1531.53 Q2281.36 1529.01 2282.24 1527.27 Q2283.15 1525.51 2283.15 1523.82 Q2283.15 1521.07 2281.2 1519.33 Q2279.28 1517.6 2276.18 1517.6 Q2273.98 1517.6 2271.53 1518.36 Q2269.1 1519.13 2266.32 1520.68 L2266.32 1515.95 Q2269.14 1514.82 2271.6 1514.24 Q2274.05 1513.66 2276.09 1513.66 Q2281.46 1513.66 2284.65 1516.35 Q2287.85 1519.03 2287.85 1523.52 Q2287.85 1525.65 2287.04 1527.57 Q2286.25 1529.47 2284.14 1532.07 Q2283.56 1532.74 2280.46 1535.95 Q2277.36 1539.15 2271.71 1544.91 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M2307.85 1517.37 Q2304.23 1517.37 2302.41 1520.93 Q2300.6 1524.47 2300.6 1531.6 Q2300.6 1538.71 2302.41 1542.27 Q2304.23 1545.82 2307.85 1545.82 Q2311.48 1545.82 2313.29 1542.27 Q2315.11 1538.71 2315.11 1531.6 Q2315.11 1524.47 2313.29 1520.93 Q2311.48 1517.37 2307.85 1517.37 M2307.85 1513.66 Q2313.66 1513.66 2316.71 1518.27 Q2319.79 1522.85 2319.79 1531.6 Q2319.79 1540.33 2316.71 1544.94 Q2313.66 1549.52 2307.85 1549.52 Q2302.04 1549.52 2298.96 1544.94 Q2295.9 1540.33 2295.9 1531.6 Q2295.9 1522.85 2298.96 1518.27 Q2302.04 1513.66 2307.85 1513.66 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip742)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  239.19,1336.78 2352.76,1336.78 \n  \"/>\n<polyline clip-path=\"url(#clip742)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  239.19,1054.6 2352.76,1054.6 \n  \"/>\n<polyline clip-path=\"url(#clip742)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  239.19,772.425 2352.76,772.425 \n  \"/>\n<polyline clip-path=\"url(#clip742)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  239.19,490.246 2352.76,490.246 \n  \"/>\n<polyline clip-path=\"url(#clip742)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:2; stroke-opacity:0.1; fill:none\" points=\"\n  239.19,208.067 2352.76,208.067 \n  \"/>\n<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  239.19,1486.45 239.19,47.2441 \n  \"/>\n<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  239.19,1336.78 258.088,1336.78 \n  \"/>\n<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  239.19,1054.6 258.088,1054.6 \n  \"/>\n<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  239.19,772.425 258.088,772.425 \n  \"/>\n<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  239.19,490.246 258.088,490.246 \n  \"/>\n<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  239.19,208.067 258.088,208.067 \n  \"/>\n<path clip-path=\"url(#clip740)\" d=\"M50.9921 1337.23 L80.6679 1337.23 L80.6679 1341.17 L50.9921 1341.17 L50.9921 1337.23 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M91.5706 1350.13 L99.2095 1350.13 L99.2095 1323.76 L90.8993 1325.43 L90.8993 1321.17 L99.1632 1319.5 L103.839 1319.5 L103.839 1350.13 L111.478 1350.13 L111.478 1354.06 L91.5706 1354.06 L91.5706 1350.13 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M120.969 1319.5 L139.325 1319.5 L139.325 1323.44 L125.251 1323.44 L125.251 1331.91 Q126.27 1331.56 127.288 1331.4 Q128.307 1331.22 129.325 1331.22 Q135.112 1331.22 138.492 1334.39 Q141.871 1337.56 141.871 1342.97 Q141.871 1348.55 138.399 1351.66 Q134.927 1354.73 128.607 1354.73 Q126.432 1354.73 124.163 1354.36 Q121.918 1353.99 119.51 1353.25 L119.51 1348.55 Q121.594 1349.69 123.816 1350.24 Q126.038 1350.8 128.515 1350.8 Q132.519 1350.8 134.857 1348.69 Q137.195 1346.59 137.195 1342.97 Q137.195 1339.36 134.857 1337.26 Q132.519 1335.15 128.515 1335.15 Q126.64 1335.15 124.765 1335.57 Q122.913 1335.98 120.969 1336.86 L120.969 1319.5 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M161.084 1322.58 Q157.473 1322.58 155.644 1326.15 Q153.839 1329.69 153.839 1336.82 Q153.839 1343.92 155.644 1347.49 Q157.473 1351.03 161.084 1351.03 Q164.718 1351.03 166.524 1347.49 Q168.353 1343.92 168.353 1336.82 Q168.353 1329.69 166.524 1326.15 Q164.718 1322.58 161.084 1322.58 M161.084 1318.88 Q166.894 1318.88 169.95 1323.48 Q173.029 1328.07 173.029 1336.82 Q173.029 1345.54 169.95 1350.15 Q166.894 1354.73 161.084 1354.73 Q155.274 1354.73 152.195 1350.15 Q149.14 1345.54 149.14 1336.82 Q149.14 1328.07 152.195 1323.48 Q155.274 1318.88 161.084 1318.88 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M191.246 1322.58 Q187.635 1322.58 185.806 1326.15 Q184.001 1329.69 184.001 1336.82 Q184.001 1343.92 185.806 1347.49 Q187.635 1351.03 191.246 1351.03 Q194.88 1351.03 196.686 1347.49 Q198.514 1343.92 198.514 1336.82 Q198.514 1329.69 196.686 1326.15 Q194.88 1322.58 191.246 1322.58 M191.246 1318.88 Q197.056 1318.88 200.112 1323.48 Q203.19 1328.07 203.19 1336.82 Q203.19 1345.54 200.112 1350.15 Q197.056 1354.73 191.246 1354.73 Q185.436 1354.73 182.357 1350.15 Q179.302 1345.54 179.302 1336.82 Q179.302 1328.07 182.357 1323.48 Q185.436 1318.88 191.246 1318.88 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M50.9921 1055.06 L80.6679 1055.06 L80.6679 1058.99 L50.9921 1058.99 L50.9921 1055.06 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M91.5706 1067.95 L99.2095 1067.95 L99.2095 1041.58 L90.8993 1043.25 L90.8993 1038.99 L99.1632 1037.32 L103.839 1037.32 L103.839 1067.95 L111.478 1067.95 L111.478 1071.88 L91.5706 1071.88 L91.5706 1067.95 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M124.95 1067.95 L141.269 1067.95 L141.269 1071.88 L119.325 1071.88 L119.325 1067.95 Q121.987 1065.19 126.57 1060.56 Q131.177 1055.91 132.357 1054.57 Q134.603 1052.05 135.482 1050.31 Q136.385 1048.55 136.385 1046.86 Q136.385 1044.11 134.441 1042.37 Q132.519 1040.63 129.418 1040.63 Q127.219 1040.63 124.765 1041.4 Q122.334 1042.16 119.557 1043.71 L119.557 1038.99 Q122.381 1037.86 124.834 1037.28 Q127.288 1036.7 129.325 1036.7 Q134.695 1036.7 137.89 1039.38 Q141.084 1042.07 141.084 1046.56 Q141.084 1048.69 140.274 1050.61 Q139.487 1052.51 137.381 1055.1 Q136.802 1055.77 133.7 1058.99 Q130.598 1062.18 124.95 1067.95 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M161.084 1040.4 Q157.473 1040.4 155.644 1043.97 Q153.839 1047.51 153.839 1054.64 Q153.839 1061.74 155.644 1065.31 Q157.473 1068.85 161.084 1068.85 Q164.718 1068.85 166.524 1065.31 Q168.353 1061.74 168.353 1054.64 Q168.353 1047.51 166.524 1043.97 Q164.718 1040.4 161.084 1040.4 M161.084 1036.7 Q166.894 1036.7 169.95 1041.31 Q173.029 1045.89 173.029 1054.64 Q173.029 1063.37 169.95 1067.97 Q166.894 1072.56 161.084 1072.56 Q155.274 1072.56 152.195 1067.97 Q149.14 1063.37 149.14 1054.64 Q149.14 1045.89 152.195 1041.31 Q155.274 1036.7 161.084 1036.7 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M191.246 1040.4 Q187.635 1040.4 185.806 1043.97 Q184.001 1047.51 184.001 1054.64 Q184.001 1061.74 185.806 1065.31 Q187.635 1068.85 191.246 1068.85 Q194.88 1068.85 196.686 1065.31 Q198.514 1061.74 198.514 1054.64 Q198.514 1047.51 196.686 1043.97 Q194.88 1040.4 191.246 1040.4 M191.246 1036.7 Q197.056 1036.7 200.112 1041.31 Q203.19 1045.89 203.19 1054.64 Q203.19 1063.37 200.112 1067.97 Q197.056 1072.56 191.246 1072.56 Q185.436 1072.56 182.357 1067.97 Q179.302 1063.37 179.302 1054.64 Q179.302 1045.89 182.357 1041.31 Q185.436 1036.7 191.246 1036.7 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M81.154 772.876 L110.83 772.876 L110.83 776.811 L81.154 776.811 L81.154 772.876 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M121.061 788.987 L121.061 784.728 Q122.82 785.561 124.626 786.001 Q126.432 786.441 128.168 786.441 Q132.797 786.441 135.228 783.339 Q137.681 780.214 138.029 773.872 Q136.686 775.862 134.626 776.927 Q132.566 777.992 130.066 777.992 Q124.881 777.992 121.848 774.867 Q118.839 771.719 118.839 766.279 Q118.839 760.955 121.987 757.738 Q125.135 754.52 130.367 754.52 Q136.362 754.52 139.51 759.126 Q142.681 763.71 142.681 772.46 Q142.681 780.631 138.793 785.515 Q134.927 790.376 128.376 790.376 Q126.617 790.376 124.811 790.029 Q123.006 789.682 121.061 788.987 M130.367 774.335 Q133.515 774.335 135.344 772.182 Q137.195 770.029 137.195 766.279 Q137.195 762.552 135.344 760.4 Q133.515 758.224 130.367 758.224 Q127.219 758.224 125.367 760.4 Q123.538 762.552 123.538 766.279 Q123.538 770.029 125.367 772.182 Q127.219 774.335 130.367 774.335 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M161.084 758.224 Q157.473 758.224 155.644 761.788 Q153.839 765.33 153.839 772.46 Q153.839 779.566 155.644 783.131 Q157.473 786.673 161.084 786.673 Q164.718 786.673 166.524 783.131 Q168.353 779.566 168.353 772.46 Q168.353 765.33 166.524 761.788 Q164.718 758.224 161.084 758.224 M161.084 754.52 Q166.894 754.52 169.95 759.126 Q173.029 763.71 173.029 772.46 Q173.029 781.186 169.95 785.793 Q166.894 790.376 161.084 790.376 Q155.274 790.376 152.195 785.793 Q149.14 781.186 149.14 772.46 Q149.14 763.71 152.195 759.126 Q155.274 754.52 161.084 754.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M191.246 758.224 Q187.635 758.224 185.806 761.788 Q184.001 765.33 184.001 772.46 Q184.001 779.566 185.806 783.131 Q187.635 786.673 191.246 786.673 Q194.88 786.673 196.686 783.131 Q198.514 779.566 198.514 772.46 Q198.514 765.33 196.686 761.788 Q194.88 758.224 191.246 758.224 M191.246 754.52 Q197.056 754.52 200.112 759.126 Q203.19 763.71 203.19 772.46 Q203.19 781.186 200.112 785.793 Q197.056 790.376 191.246 790.376 Q185.436 790.376 182.357 785.793 Q179.302 781.186 179.302 772.46 Q179.302 763.71 182.357 759.126 Q185.436 754.52 191.246 754.52 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M81.154 490.698 L110.83 490.698 L110.83 494.633 L81.154 494.633 L81.154 490.698 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M131.501 488.383 Q128.353 488.383 126.501 490.536 Q124.672 492.688 124.672 496.438 Q124.672 500.165 126.501 502.341 Q128.353 504.494 131.501 504.494 Q134.649 504.494 136.478 502.341 Q138.33 500.165 138.33 496.438 Q138.33 492.688 136.478 490.536 Q134.649 488.383 131.501 488.383 M140.783 473.73 L140.783 477.989 Q139.024 477.156 137.219 476.716 Q135.436 476.276 133.677 476.276 Q129.047 476.276 126.594 479.401 Q124.163 482.526 123.816 488.846 Q125.182 486.832 127.242 485.767 Q129.302 484.679 131.779 484.679 Q136.987 484.679 139.996 487.85 Q143.029 490.998 143.029 496.438 Q143.029 501.762 139.881 504.98 Q136.732 508.197 131.501 508.197 Q125.506 508.197 122.334 503.614 Q119.163 499.008 119.163 490.281 Q119.163 482.086 123.052 477.225 Q126.941 472.341 133.492 472.341 Q135.251 472.341 137.033 472.688 Q138.839 473.036 140.783 473.73 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M161.084 476.045 Q157.473 476.045 155.644 479.61 Q153.839 483.151 153.839 490.281 Q153.839 497.387 155.644 500.952 Q157.473 504.494 161.084 504.494 Q164.718 504.494 166.524 500.952 Q168.353 497.387 168.353 490.281 Q168.353 483.151 166.524 479.61 Q164.718 476.045 161.084 476.045 M161.084 472.341 Q166.894 472.341 169.95 476.948 Q173.029 481.531 173.029 490.281 Q173.029 499.008 169.95 503.614 Q166.894 508.197 161.084 508.197 Q155.274 508.197 152.195 503.614 Q149.14 499.008 149.14 490.281 Q149.14 481.531 152.195 476.948 Q155.274 472.341 161.084 472.341 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M191.246 476.045 Q187.635 476.045 185.806 479.61 Q184.001 483.151 184.001 490.281 Q184.001 497.387 185.806 500.952 Q187.635 504.494 191.246 504.494 Q194.88 504.494 196.686 500.952 Q198.514 497.387 198.514 490.281 Q198.514 483.151 196.686 479.61 Q194.88 476.045 191.246 476.045 M191.246 472.341 Q197.056 472.341 200.112 476.948 Q203.19 481.531 203.19 490.281 Q203.19 499.008 200.112 503.614 Q197.056 508.197 191.246 508.197 Q185.436 508.197 182.357 503.614 Q179.302 499.008 179.302 490.281 Q179.302 481.531 182.357 476.948 Q185.436 472.341 191.246 472.341 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M81.154 208.519 L110.83 208.519 L110.83 212.454 L81.154 212.454 L81.154 208.519 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M135.089 206.713 Q138.445 207.431 140.32 209.699 Q142.218 211.968 142.218 215.301 Q142.218 220.417 138.7 223.218 Q135.181 226.019 128.7 226.019 Q126.524 226.019 124.209 225.579 Q121.918 225.162 119.464 224.306 L119.464 219.792 Q121.408 220.926 123.723 221.505 Q126.038 222.084 128.561 222.084 Q132.959 222.084 135.251 220.347 Q137.566 218.611 137.566 215.301 Q137.566 212.246 135.413 210.533 Q133.283 208.797 129.464 208.797 L125.436 208.797 L125.436 204.954 L129.649 204.954 Q133.098 204.954 134.927 203.588 Q136.756 202.199 136.756 199.607 Q136.756 196.945 134.857 195.533 Q132.982 194.098 129.464 194.098 Q127.543 194.098 125.344 194.514 Q123.145 194.931 120.506 195.811 L120.506 191.644 Q123.168 190.903 125.482 190.533 Q127.82 190.162 129.881 190.162 Q135.205 190.162 138.306 192.593 Q141.408 195 141.408 199.121 Q141.408 201.991 139.765 203.982 Q138.121 205.949 135.089 206.713 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M161.084 193.866 Q157.473 193.866 155.644 197.431 Q153.839 200.973 153.839 208.102 Q153.839 215.209 155.644 218.773 Q157.473 222.315 161.084 222.315 Q164.718 222.315 166.524 218.773 Q168.353 215.209 168.353 208.102 Q168.353 200.973 166.524 197.431 Q164.718 193.866 161.084 193.866 M161.084 190.162 Q166.894 190.162 169.95 194.769 Q173.029 199.352 173.029 208.102 Q173.029 216.829 169.95 221.435 Q166.894 226.019 161.084 226.019 Q155.274 226.019 152.195 221.435 Q149.14 216.829 149.14 208.102 Q149.14 199.352 152.195 194.769 Q155.274 190.162 161.084 190.162 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M191.246 193.866 Q187.635 193.866 185.806 197.431 Q184.001 200.973 184.001 208.102 Q184.001 215.209 185.806 218.773 Q187.635 222.315 191.246 222.315 Q194.88 222.315 196.686 218.773 Q198.514 215.209 198.514 208.102 Q198.514 200.973 196.686 197.431 Q194.88 193.866 191.246 193.866 M191.246 190.162 Q197.056 190.162 200.112 194.769 Q203.19 199.352 203.19 208.102 Q203.19 216.829 200.112 221.435 Q197.056 226.019 191.246 226.019 Q185.436 226.019 182.357 221.435 Q179.302 216.829 179.302 208.102 Q179.302 199.352 182.357 194.769 Q185.436 190.162 191.246 190.162 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><polyline clip-path=\"url(#clip742)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  299.008,1203.88 403.952,886.036 508.896,901.176 613.839,662.709 718.783,695.714 823.727,867.08 928.67,1445.72 1033.61,982.764 1138.56,748.723 1243.5,1127.72 \n  1348.44,554.704 1453.39,1037.06 1558.33,967.656 1663.28,818.853 1768.22,1050.06 1873.16,158.669 1978.11,1051 2083.05,1036.97 2187.99,113.092 2292.94,87.9763 \n  \n  \"/>\n<path clip-path=\"url(#clip740)\" d=\"\nM309.643 198.898 L573.429 198.898 L573.429 95.2176 L309.643 95.2176  Z\n  \" fill=\"#ffffff\" fill-rule=\"evenodd\" fill-opacity=\"1\"/>\n<polyline clip-path=\"url(#clip740)\" style=\"stroke:#000000; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  309.643,198.898 573.429,198.898 573.429,95.2176 309.643,95.2176 309.643,198.898 \n  \"/>\n<polyline clip-path=\"url(#clip740)\" style=\"stroke:#009af9; stroke-linecap:round; stroke-linejoin:round; stroke-width:4; stroke-opacity:1; fill:none\" points=\"\n  333.127,147.058 474.031,147.058 \n  \"/>\n<path clip-path=\"url(#clip740)\" d=\"M511.358 166.745 Q509.552 171.375 507.839 172.787 Q506.126 174.199 503.256 174.199 L499.853 174.199 L499.853 170.634 L502.353 170.634 Q504.112 170.634 505.084 169.8 Q506.057 168.967 507.237 165.865 L508.001 163.921 L497.515 138.412 L502.029 138.412 L510.131 158.689 L518.233 138.412 L522.746 138.412 L511.358 166.745 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /><path clip-path=\"url(#clip740)\" d=\"M530.038 160.402 L537.677 160.402 L537.677 134.037 L529.367 135.703 L529.367 131.444 L537.631 129.778 L542.306 129.778 L542.306 160.402 L549.945 160.402 L549.945 164.338 L530.038 164.338 L530.038 160.402 Z\" fill=\"#000000\" fill-rule=\"evenodd\" fill-opacity=\"1\" /></svg>\n"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Plot the stuff\n",
    "plot(episode_test_reward_hook.rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fbbe4c",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "close_visualization(); # closes the MeshCat visualization"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
