{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0649b76a",
   "metadata": {},
   "source": [
    "# Init Bionic VTOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96751412",
   "metadata": {},
   "outputs": [],
   "source": [
    "include(\"../Flyonic.jl\");\n",
    "using .Flyonic; #simulator kram\n",
    "\n",
    "using Rotations; # used for initial position\n",
    "\n",
    "#find packages on julia hub\n",
    "using ReinforcementLearning; \n",
    "using StableRNGs;\n",
    "using Flux; #DL\n",
    "using Flux.Losses;\n",
    "using Random;\n",
    "using IntervalSets;\n",
    "using LinearAlgebra;\n",
    "using Distributions;\n",
    "\n",
    "using Plots;\n",
    "using Statistics;\n",
    "\n",
    "using BSON: @save, @load # save and load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7e4ee64",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m\u001B[1m┌ \u001B[22m\u001B[39m\u001B[36m\u001B[1mInfo: \u001B[22m\u001B[39mMeshCat server started. You can open the visualizer by visiting the following URL in your browser:\n",
      "\u001B[36m\u001B[1m└ \u001B[22m\u001B[39mhttp://127.0.0.1:8700\n"
     ]
    }
   ],
   "source": [
    "create_visualization(); #from flyonics package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d9557df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# indicates how many threads Julia was started with. This is important for the multi-threaded environment\n",
    "Threads.nthreads()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5411db62",
   "metadata": {},
   "source": [
    "# Create Reinforcement Learning Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96af6ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#connect RL-library with simulation via this environment\n",
    "mutable struct VtolEnv{A,T,ACT,R<:AbstractRNG} <: AbstractEnv # Parametric Constructor for a subtype of AbstractEnv\n",
    "    #required\n",
    "    action_space::A\n",
    "    observation_space::Space{Vector{ClosedInterval{T}}}\n",
    "    state::Vector{T} #state of system; goes in policy\n",
    "    action::ACT #action that system does next\n",
    "    done::Bool #e.g. drone crashed\n",
    "    t::T\n",
    "    rng::R\n",
    "\n",
    "    name::String #for multible environoments\n",
    "    visualization::Bool\n",
    "    realtime::Bool # realtime\n",
    "    \n",
    "    # Everything you need aditionaly can also go in here.\n",
    "    #additional states for simulation; not for policy\n",
    "    x_W::Vector{T}\n",
    "    v_B::Vector{T}\n",
    "    R_W::Matrix{T}\n",
    "    ω_B::Vector{T}\n",
    "    wind_W::Vector{T}\n",
    "    Δt::T\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14a6873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a keyword-based constructor for the type declared in the mutable struct typedef. \n",
    "# It could also be done with the macro Base.@kwdef.\n",
    "function VtolEnv(;\n",
    "     \n",
    "    #continuous = true,\n",
    "    rng = Random.GLOBAL_RNG, # Random number generation\n",
    "    name = \"vtol\",\n",
    "    visualization = false,\n",
    "    realtime = false, # realtime\n",
    "    kwargs... # let the function take an arbitrary number of keyword arguments \n",
    ")\n",
    "    \n",
    "    T = Float64; # explicit type which is used e.g. in state. Cannot be altered due to the poor matrix defininon.\n",
    "\n",
    "    #action_space = Base.OneTo(21) # 21 discrete positions for the flaps\n",
    "    \n",
    "    #here: two actions; continuous\n",
    "    #Beginning: stay 2D\n",
    "    #Later: 4D: independent rotors and flaps (thats all actuators)\n",
    "    action_space = Space(\n",
    "        ClosedInterval{T}[\n",
    "            0.0..2.0, # thrust\n",
    "            -1.0..1.0, # flaps\n",
    "            ], \n",
    "    )\n",
    "\n",
    "    #reduced to 2D for now\n",
    "    state_space = Space( # Three continuous values in state space.\n",
    "        ClosedInterval{T}[\n",
    "            typemin(T)..typemax(T), # rotation arround y\n",
    "            typemin(T)..typemax(T), # rotation velocity arround y\n",
    "            typemin(T)..typemax(T), # world position along x\n",
    "            typemin(T)..typemax(T), # world position along z\n",
    "            ], \n",
    "    )\n",
    "    \n",
    "    if visualization\n",
    "        create_VTOL(name, actuators = true, color_vec=[1.0; 1.0; 0.6; 1.0]); #for viz\n",
    "        set_transform(name, [0.0; 0.0; 0.0] ,QuatRotation(UnitQuaternion(RotY(-pi/2.0)*RotX(pi)))); #for viz\n",
    "        set_actuators(name, [0.0; 0.0; 0.0; 0.0]) #for viz\n",
    "    end\n",
    "\n",
    "    #instantiates the sctruct from before\n",
    "    environment = VtolEnv(\n",
    "        action_space,\n",
    "        state_space,\n",
    "        zeros(T, length(state_space)), # current state, needs to be extended.\n",
    "        rand(action_space),\n",
    "        false, # episode done ?\n",
    "        0.0, # time\n",
    "        rng, # random number generator  \n",
    "        name,\n",
    "        visualization,\n",
    "        realtime,\n",
    "        zeros(T, 3), # x_W\n",
    "        zeros(T, 3), # v_B\n",
    "        Matrix(UnitQuaternion(RotY(-pi/2.0)*RotX(pi))), # Float64... so T needs to be Float64\n",
    "        zeros(T, 3), # ω_B\n",
    "        zeros(T, 3), # wind_W\n",
    "        T(0.025), # Δt  \n",
    "    )\n",
    "    \n",
    "    #do this for simulation start\n",
    "    reset!(environment)\n",
    "    \n",
    "    return environment\n",
    "    \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec660d5e",
   "metadata": {},
   "source": [
    "Just for explanation:\n",
    "\n",
    "1. A mutable Struct is created. A struct is a constructor and a constructor is a function that creates new objects.\n",
    "2. A outer keyword-based constructor method is added for the type declared in the mutable struct typedef before.\n",
    "\n",
    "So now we have a function with two methods. Julia will decide which method to call by multiple dispatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa93cbfe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "# 2 methods for type constructor:\n[1] VtolEnv(; rng, name, visualization, realtime, kwargs...) in Main at In[5]:3\n[2] VtolEnv(action_space::A, observation_space::Space{Array{ClosedInterval{T}, 1}}, state::Vector{T}, action::ACT, done::Bool, t::T, rng::R, name::String, visualization::Bool, realtime::Bool, x_W::Vector{T}, v_B::Vector{T}, R_W::Matrix{T}, ω_B::Vector{T}, wind_W::Vector{T}, Δt::T) where {A, T, ACT, R<:AbstractRNG} in Main at In[4]:4",
      "text/html": "# 2 methods for type constructor:<ul><li> VtolEnv(; <i>rng, name, visualization, realtime, kwargs...</i>) in Main at In[5]:3</li> <li> VtolEnv(action_space::<b>A</b>, observation_space::<b>Space{Array{ClosedInterval{T}, 1}}</b>, state::<b>Vector{T}</b>, action::<b>ACT</b>, done::<b>Bool</b>, t::<b>T</b>, rng::<b>R</b>, name::<b>String</b>, visualization::<b>Bool</b>, realtime::<b>Bool</b>, x_W::<b>Vector{T}</b>, v_B::<b>Vector{T}</b>, R_W::<b>Matrix{T}</b>, ω_B::<b>Vector{T}</b>, wind_W::<b>Vector{T}</b>, Δt::<b>T</b>)<i> where {A, T, ACT, R<:AbstractRNG}</i> in Main at In[4]:4</li> </ul>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods(VtolEnv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806413d1",
   "metadata": {},
   "source": [
    "# Define the RL interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f822029",
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(env::VtolEnv, seed) = Random.seed!(env.rng, seed)\n",
    "RLBase.action_space(env::VtolEnv) = env.action_space\n",
    "RLBase.state_space(env::VtolEnv) = env.observation_space\n",
    "RLBase.is_terminated(env::VtolEnv) = env.done\n",
    "RLBase.state(env::VtolEnv) = env.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1f7fb89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "function computeReward(env::VtolEnv{A,T}) where {A,T}\n",
    "    \n",
    "    #this rewards makes drone go straight up :)\n",
    "    stay_alive = 3.0\n",
    "    not_upright_orientation = abs(env.state[1]-pi*0.5)*10.0\n",
    "    not_centered_position = abs(env.state[3])*10.0\n",
    "    hight = env.state[4]*100.0\n",
    "    \n",
    "    return stay_alive - not_upright_orientation - not_centered_position + hight\n",
    "end\n",
    "\n",
    "\n",
    "RLBase.reward(env::VtolEnv{A,T}) where {A,T} = computeReward(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae45ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "function RLBase.reset!(env::VtolEnv{A,T}) where {A,T}\n",
    "    \n",
    "    # Visualize initial state\n",
    "    if env.visualization\n",
    "        set_transform(env.name, env.x_W,QuatRotation(env.R_W));\n",
    "        set_actuators(env.name, [0.0; 0.0; 0.0; 0.0])\n",
    "    end\n",
    "        \n",
    "    env.x_W = [0.0; 0.0; 0.0];\n",
    "    env.v_B = [0.0; 0.0; 0.0];\n",
    "    env.R_W = Matrix(UnitQuaternion(RotY(-pi/2.0)*RotX(pi)));\n",
    "    env.ω_B = [0.0; 0.0; 0.0];\n",
    "    env.wind_W = [0.0; 0.0; 0.0]; #use this for gusty condition\n",
    "\n",
    " \n",
    "    env.state = [env.ω_B[2]; Rotations.params(RotYXZ(env.R_W))[1]; env.x_W[1]; env.x_W[3]]\n",
    "    env.t = 0.0\n",
    "    env.action = [0.0]\n",
    "    env.done = false\n",
    "    nothing\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cf1a7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defines a methods for a callable object.\n",
    "# So when a VtolEnv object is created, it has this method that can be called\n",
    "# Apply chosen actions on simulator\n",
    "# add a third function to environment with action a\n",
    "function (env::VtolEnv)(a)\n",
    "\n",
    "    # set the propeller trust and the two flaps 2D case\n",
    "    next_action = [a[1], a[1], a[2], a[2]]\n",
    "   \n",
    "    _step!(env, next_action)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "26a5a0da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "# VtolEnv\n\n## Traits\n\n| Trait Type        |                  Value |\n|:----------------- | ----------------------:|\n| NumAgentStyle     |          SingleAgent() |\n| DynamicStyle      |           Sequential() |\n| InformationStyle  | ImperfectInformation() |\n| ChanceStyle       |           Stochastic() |\n| RewardStyle       |           StepReward() |\n| UtilityStyle      |           GeneralSum() |\n| ActionStyle       |     MinimalActionSet() |\n| StateStyle        |     Observation{Any}() |\n| DefaultStateStyle |     Observation{Any}() |\n\n## Is Environment Terminated?\n\nNo\n\n## State Space\n\n`Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[-Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf])`\n\n## Action Space\n\n`Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[0.0..2.0, -1.0..1.0])`\n\n## Current State\n\n```\n[0.0, 1.5707963267948966, 0.0, 0.0]\n```\n",
      "text/markdown": "# VtolEnv\n\n## Traits\n\n| Trait Type        |                  Value |\n|:----------------- | ----------------------:|\n| NumAgentStyle     |          SingleAgent() |\n| DynamicStyle      |           Sequential() |\n| InformationStyle  | ImperfectInformation() |\n| ChanceStyle       |           Stochastic() |\n| RewardStyle       |           StepReward() |\n| UtilityStyle      |           GeneralSum() |\n| ActionStyle       |     MinimalActionSet() |\n| StateStyle        |     Observation{Any}() |\n| DefaultStateStyle |     Observation{Any}() |\n\n## Is Environment Terminated?\n\nNo\n\n## State Space\n\n`Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[-Inf..Inf, -Inf..Inf, -Inf..Inf, -Inf..Inf])`\n\n## Action Space\n\n`Space{Vector{ClosedInterval{Float64}}}(ClosedInterval{Float64}[0.0..2.0, -1.0..1.0])`\n\n## Current State\n\n```\n[0.0, 1.5707963267948966, 0.0, 0.0]\n```\n"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = VtolEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5aa39474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "# 3 methods for callable object:\n[1] (env::VtolEnv)(a) in Main at In[10]:5\n[2] (env::AbstractEnv)(action) in ReinforcementLearningBase\n[3] (env::AbstractEnv)(action, player) in ReinforcementLearningBase",
      "text/html": "# 3 methods for callable object:<ul><li> (env::<b>VtolEnv</b>)(a) in Main at In[10]:5</li> <li> (env::<b>AbstractEnv</b>)(action) in ReinforcementLearningBase</li> <li> (env::<b>AbstractEnv</b>)(action, player) in ReinforcementLearningBase</li> </ul>"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "methods(env) # Just to explain which methods the object has"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e7d4727",
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual step for each simulation iteration\n",
    "# Trajectory: generate externally; and then do some calculation in the step; in the environment add additional state: v_soll; v_ist --> use in reward function\n",
    "# Generate trajectories randomly, so that iterations train on different trajectories\n",
    "# Test set: always use same trajectory\n",
    "# TODO maybe we can get trajectory generation from last year\n",
    "# Evaluation metric: how well do I track my velocity trajectory? Look for 2-3 metrics here and thats ok\n",
    "function _step!(env::VtolEnv, next_action)\n",
    "        \n",
    "    # caluclate wind impact\n",
    "    v_in_wind_B = vtol_add_wind(env.v_B, env.R_W, env.wind_W)\n",
    "    # caluclate aerodynamic forces\n",
    "    torque_B, force_B = vtol_model(v_in_wind_B, next_action, eth_vtol_param);\n",
    "    # integrate rigid body dynamics for Δt\n",
    "    # W-world KOS; B-body KOS; Simulation is already complete\n",
    "    env.x_W, env.v_B, env.R_W, env.ω_B, time = rigid_body_simple(torque_B, force_B, env.x_W, env.v_B, env.R_W, env.ω_B, env.t, env.Δt, eth_vtol_param)\n",
    "\n",
    "\n",
    "    if env.realtime\n",
    "        sleep(env.Δt); # just a dirty hack. this is of course slower than real time.\n",
    "    end\n",
    "    \n",
    "    # Visualize the new state \n",
    "    # TODO: Can be removed for real trainings\n",
    "    if env.visualization\n",
    "        set_transform(env.name, env.x_W, QuatRotation(env.R_W));\n",
    "        set_actuators(env.name, next_action)\n",
    "    end\n",
    " \n",
    "    env.t += env.Δt\n",
    "    \n",
    "    # State space\n",
    "    # Pass more states to RF-Learning-Agent, if required\n",
    "    rot = Rotations.params(RotYXZ(env.R_W))[1]\n",
    "    env.state[1] = rot # rotation arround y\n",
    "    env.state[2] = env.ω_B[2] # rotation velocity arround y\n",
    "    env.state[3] = env.x_W[1] # world position along x\n",
    "    env.state[4] = env.x_W[3] # world position along z\n",
    "    \n",
    "    \n",
    "    # Termination criteria\n",
    "    env.done =\n",
    "        #norm(v_B) > 2.0 || # stop if body is too fast\n",
    "        env.x_W[3] < -1.0 || # stop if body is below -1m\n",
    "        0.0 > rot || # Stop if the drone is pitched 90°.\n",
    "        rot > pi || # Stop if the drone is pitched 90°.\n",
    "        env.t > 10 # stop after 10s\n",
    "    nothing #return nothing\n",
    "end;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6e1cd988",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[0m\u001B[1mTest Summary:              | \u001B[22m\u001B[32m\u001B[1mPass  \u001B[22m\u001B[39m\u001B[36m\u001B[1mTotal  \u001B[22m\u001B[39m\u001B[0m\u001B[1mTime\u001B[22m\n",
      "random policy with VtolEnv | \u001B[32m2000  \u001B[39m\u001B[36m 2000  \u001B[39m\u001B[0m1.0s\n"
     ]
    },
    {
     "data": {
      "text/plain": "Test.DefaultTestSet(\"random policy with VtolEnv\", Any[], 2000, false, false, true, 1.673884352576474e9, 1.673884353612373e9)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RLBase.test_runnable!(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c223a31f",
   "metadata": {},
   "source": [
    "Show an overview of the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe6de74",
   "metadata": {},
   "source": [
    "# Setup of a reinforcement learning experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5683fe8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "MultiThreadEnv(8 x VtolEnv)",
      "text/markdown": "MultiThreadEnv(8 x VtolEnv)"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 123    \n",
    "rng = StableRNG(seed)\n",
    "    N_ENV = 8 #number of envs\n",
    "    UPDATE_FREQ = 1024\n",
    "    \n",
    "    \n",
    "    # define multiple environments for parallel training\n",
    "    env = MultiThreadEnv([\n",
    "        # use different names for the visualization\n",
    "        VtolEnv(; rng = StableRNG(hash(seed+i)), name = \"vtol$i\", visualization = false) for i in 1:N_ENV\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d1f128b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function approximator\n",
    "    ns, na = length(state(env[1])), length(action_space(env[1]))\n",
    "    #ActorCritic Policy\n",
    "    approximator = ActorCritic(\n",
    "                #ns - number states as input\n",
    "                #3 layer; last layer splitted in mean and variance; then action is sampled\n",
    "                actor = GaussianNetwork(\n",
    "                    pre = Chain(\n",
    "                    Dense(ns, 16, relu; initW = glorot_uniform(rng)),#\n",
    "                    Dense(16, 16, relu; initW = glorot_uniform(rng)),\n",
    "                    ),\n",
    "                    μ = Chain(Dense(16, na; initW = glorot_uniform(rng))),\n",
    "                    logσ = Chain(Dense(16, na; initW = glorot_uniform(rng))),\n",
    "                ),\n",
    "                critic = Chain(\n",
    "                    Dense(ns, 16, relu; initW = glorot_uniform(rng)),\n",
    "                    Dense(16, 16, relu; initW = glorot_uniform(rng)),\n",
    "                    Dense(16, 1; initW = glorot_uniform(rng)),\n",
    "                ),\n",
    "                optimizer = ADAM(1e-3),\n",
    "            );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ea4c37c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[36m\u001B[1m┌ \u001B[22m\u001B[39m\u001B[36m\u001B[1mInfo: \u001B[22m\u001B[39mThe GPU function is being called but the GPU is not accessible. \n",
      "\u001B[36m\u001B[1m└ \u001B[22m\u001B[39mDefaulting back to the CPU. (No action is required if you want to run on the CPU).\n"
     ]
    },
    {
     "data": {
      "text/plain": "typename(Agent)\n├─ policy => typename(PPOPolicy)\n│  ├─ approximator => typename(ActorCritic)\n│  │  ├─ actor => typename(GaussianNetwork)\n│  │  │  ├─ pre => typename(Chain)\n│  │  │  │  └─ layers\n│  │  │  │     ├─ 1\n│  │  │  │     │  └─ typename(Dense)\n│  │  │  │     │     ├─ weight => 16×4 Matrix{Float32}\n│  │  │  │     │     ├─ bias => 16-element Vector{Float32}\n│  │  │  │     │     └─ σ => typename(typeof(relu))\n│  │  │  │     └─ 2\n│  │  │  │        └─ typename(Dense)\n│  │  │  │           ├─ weight => 16×16 Matrix{Float32}\n│  │  │  │           ├─ bias => 16-element Vector{Float32}\n│  │  │  │           └─ σ => typename(typeof(relu))\n│  │  │  ├─ μ => typename(Chain)\n│  │  │  │  └─ layers\n│  │  │  │     └─ 1\n│  │  │  │        └─ typename(Dense)\n│  │  │  │           ├─ weight => 2×16 Matrix{Float32}\n│  │  │  │           ├─ bias => 2-element Vector{Float32}\n│  │  │  │           └─ σ => typename(typeof(identity))\n│  │  │  ├─ logσ => typename(Chain)\n│  │  │  │  └─ layers\n│  │  │  │     └─ 1\n│  │  │  │        └─ typename(Dense)\n│  │  │  │           ├─ weight => 2×16 Matrix{Float32}\n│  │  │  │           ├─ bias => 2-element Vector{Float32}\n│  │  │  │           └─ σ => typename(typeof(identity))\n│  │  │  ├─ min_σ => 0.0\n│  │  │  ├─ max_σ => Inf\n│  │  │  └─ normalizer => typename(typeof(tanh))\n│  │  ├─ critic => typename(Chain)\n│  │  │  └─ layers\n│  │  │     ├─ 1\n│  │  │     │  └─ typename(Dense)\n│  │  │     │     ├─ weight => 16×4 Matrix{Float32}\n│  │  │     │     ├─ bias => 16-element Vector{Float32}\n│  │  │     │     └─ σ => typename(typeof(relu))\n│  │  │     ├─ 2\n│  │  │     │  └─ typename(Dense)\n│  │  │     │     ├─ weight => 16×16 Matrix{Float32}\n│  │  │     │     ├─ bias => 16-element Vector{Float32}\n│  │  │     │     └─ σ => typename(typeof(relu))\n│  │  │     └─ 3\n│  │  │        └─ typename(Dense)\n│  │  │           ├─ weight => 1×16 Matrix{Float32}\n│  │  │           ├─ bias => 1-element Vector{Float32}\n│  │  │           └─ σ => typename(typeof(identity))\n│  │  └─ optimizer => typename(ADAM)\n│  │     ├─ eta => 0.001\n│  │     ├─ beta\n│  │     │  ├─ 1\n│  │     │  │  └─ 0.9\n│  │     │  └─ 2\n│  │     │     └─ 0.999\n│  │     ├─ epsilon => 1.0e-8\n│  │     └─ state => typename(IdDict)\n│  ├─ γ => 0.99\n│  ├─ λ => 0.95\n│  ├─ clip_range => 0.2\n│  ├─ max_grad_norm => 0.5\n│  ├─ n_microbatches => 4\n│  ├─ n_epochs => 4\n│  ├─ actor_loss_weight => 1.0\n│  ├─ critic_loss_weight => 0.5\n│  ├─ entropy_loss_weight => 0.01\n│  ├─ rng => typename(Random._GLOBAL_RNG)\n│  ├─ n_random_start => 0\n│  ├─ update_freq => 1024\n│  ├─ update_step => 0\n│  ├─ norm => 4×4 Matrix{Float32}\n│  ├─ actor_loss => 4×4 Matrix{Float32}\n│  ├─ critic_loss => 4×4 Matrix{Float32}\n│  ├─ entropy_loss => 4×4 Matrix{Float32}\n│  └─ loss => 4×4 Matrix{Float32}\n└─ trajectory => typename(Trajectory)\n   └─ traces => typename(NamedTuple)\n      ├─ action_log_prob => 8×0 CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}\n      ├─ state => 4×8×0 CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}\n      ├─ action => 2×8×0 CircularArrayBuffers.CircularArrayBuffer{Float64, 3, Array{Float64, 3}}\n      ├─ reward => 8×0 CircularArrayBuffers.CircularArrayBuffer{Float64, 2, Matrix{Float64}}\n      └─ terminal => 8×0 CircularArrayBuffers.CircularArrayBuffer{Bool, 2, Matrix{Bool}}\n"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    #learning\n",
    "    agent = Agent( # A wrapper of an AbstractPolicy\n",
    "        # AbstractPolicy: the policy to use\n",
    "        policy = PPOPolicy(;\n",
    "                    approximator = approximator |> gpu,\n",
    "                    update_freq=UPDATE_FREQ,\n",
    "                    dist = Normal,\n",
    "                    # For parameters visit the docu: https://juliareinforcementlearning.org/docs/rlzoo/#ReinforcementLearningZoo.PPOPolicy\n",
    "                    ),\n",
    "        \n",
    "        # AbstractTrajectory: used to store transitions between an agent and an environment source\n",
    "        # depends on RL-Algorithm\n",
    "        trajectory = PPOTrajectory(;\n",
    "            capacity = UPDATE_FREQ,\n",
    "            state = Matrix{Float64} => (ns, N_ENV),\n",
    "            action = Matrix{Float64} => (na, N_ENV),\n",
    "            action_log_prob = Vector{Float64} => (N_ENV,),\n",
    "            reward = Vector{Float64} => (N_ENV,),\n",
    "            terminal = Vector{Bool} => (N_ENV,),\n",
    "        ),\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7f158a93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "saveModel (generic function with 1 method)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function saveModel(t, agent, env)\n",
    "    model = cpu(agent.policy.approximator)   \n",
    "    f = joinpath(\"./RL_models/\", \"vtol_ppo_2_$t.bson\")\n",
    "    @save f model\n",
    "    println(\"parameters at step $t saved to $f\")\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "289acdd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "loadModel (generic function with 1 method)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function loadModel()\n",
    "    # TODO use correct relative path here\n",
    "    f = joinpath(\"./RL_models/\", \"vtol_ppo_2_9320000.bson\")\n",
    "    @load f model\n",
    "    return model\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7e4abc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "function validate_policy(t, agent, env)\n",
    "    run(agent.policy, test_env, StopAfterEpisode(1), episode_test_reward_hook)\n",
    "    # the result of the hook\n",
    "    println(\"test reward at step $t: $(episode_test_reward_hook.rewards[end])\")\n",
    "    \n",
    "end;\n",
    "\n",
    "episode_test_reward_hook = TotalRewardPerEpisode(;is_display_on_exit=false)\n",
    "# create a env only for reward test\n",
    "test_env = VtolEnv(;name = \"testVTOL\", visualization = true, realtime = true);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f0c9d222",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use pretrained model\n",
    "#agent.policy.approximator = loadModel();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb737010",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:   1%|▍                                        |  ETA: 0:55:32\u001B[39m39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 10000: -899.1738639898911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:   2%|▋                                        |  ETA: 0:34:46\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 20000: -1365.9739281720017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:   3%|█▎                                       |  ETA: 0:19:54\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 30000: -1242.7983338250456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:   4%|█▌                                       |  ETA: 0:17:00\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 40000: -1384.147594818335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:   5%|██                                       |  ETA: 0:13:13\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 50000: 16010.00875925086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:   6%|██▎                                      |  ETA: 0:12:28\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 60000: 15133.175797818838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:   7%|██▉                                      |  ETA: 0:10:54\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 70000: 9839.027231205251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:   8%|███▏                                     |  ETA: 0:10:28\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 80000: 10868.1229459878\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:   9%|███▋                                     |  ETA: 0:09:16\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 90000: 20929.20980738508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  10%|████                                     |  ETA: 0:09:06\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 100000 saved to ./RL_models/vtol_ppo_2_100000.bson\n",
      "test reward at step 100000: 12441.889907208122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  11%|████▌                                    |  ETA: 0:09:01\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 110000: 20316.530804599442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  12%|████▉                                    |  ETA: 0:08:30\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 120000: 23865.10138993155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  13%|█████▏                                   |  ETA: 0:08:21\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 130000: 13008.557555647403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  14%|█████▊                                   |  ETA: 0:07:46\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 140000: 25552.644401255668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  15%|██████                                   |  ETA: 0:07:39\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 150000: 10492.772356690894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  15%|██████▍                                  |  ETA: 0:07:30\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 160000: 16340.189805970855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  17%|██████▉                                  |  ETA: 0:07:03\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 170000: 14650.067733976613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  18%|███████▎                                 |  ETA: 0:06:56\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 180000: 13051.946339558792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  18%|███████▌                                 |  ETA: 0:06:49\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 190000: 10594.273021932991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  20%|████████▏                                |  ETA: 0:06:27\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 200000 saved to ./RL_models/vtol_ppo_2_200000.bson\n",
      "test reward at step 200000: 5015.997261348494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  21%|████████▌                                |  ETA: 0:06:19\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 210000: 8991.319726428703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  21%|████████▊                                |  ETA: 0:06:11\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 220000: 19783.292270381942\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  23%|█████████▍                               |  ETA: 0:05:54\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 230000: 25845.18820322162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  24%|█████████▋                               |  ETA: 0:05:51\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 240000: 6870.9650506670005\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  24%|██████████                               |  ETA: 0:05:44\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 250000: 5064.795174660288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  26%|██████████▋                              |  ETA: 0:05:28\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 260000: 3199.4734146870815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  27%|██████████▉                              |  ETA: 0:05:22\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 270000: 5809.57831008215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  27%|███████████▎                             |  ETA: 0:05:16\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 280000: 8574.356905885286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  29%|███████████▉                             |  ETA: 0:05:02\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 290000: 5804.154143822808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  30%|████████████▏                            |  ETA: 0:04:57\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 300000 saved to ./RL_models/vtol_ppo_2_300000.bson\n",
      "test reward at step 300000: 12456.693647061242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  30%|████████████▌                            |  ETA: 0:04:53\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 310000: 13540.87725868163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  32%|█████████████▏                           |  ETA: 0:04:44\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 320000: 9920.843411107458\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  33%|█████████████▍                           |  ETA: 0:04:36\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 330000: 15072.140735328621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  33%|█████████████▊                           |  ETA: 0:04:33\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 340000: 22119.097280283753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  35%|██████████████▎                          |  ETA: 0:04:24\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 350000: 27682.33407738047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  36%|██████████████▋                          |  ETA: 0:04:22\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 360000: 20489.652039283337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  37%|███████████████▎                         |  ETA: 0:04:17\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 370000: 26766.18415859122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  38%|███████████████▌                         |  ETA: 0:04:10\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 380000: 15086.142472647965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  38%|███████████████▊                         |  ETA: 0:04:08\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 390000: 15162.276511264747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  40%|████████████████▍                        |  ETA: 0:03:59\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 400000 saved to ./RL_models/vtol_ppo_2_400000.bson\n",
      "test reward at step 400000: 24643.803175338522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  41%|████████████████▋                        |  ETA: 0:03:56\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 410000: 38749.05506559511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  41%|█████████████████                        |  ETA: 0:03:54\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 420000: 14917.204592906934\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  43%|█████████████████▋                       |  ETA: 0:03:45\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 430000: 17893.60160139251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  44%|██████████████████                       |  ETA: 0:03:43\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 440000: 21431.791105666904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  45%|██████████████████▍                      |  ETA: 0:03:37\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 450000: 21427.035511751303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  45%|██████████████████▋                      |  ETA: 0:03:35\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 460000: 25204.237228380876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  47%|███████████████████▎                     |  ETA: 0:03:28\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 470000: 23806.22346180418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  48%|███████████████████▌                     |  ETA: 0:03:26\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 480000: 33944.639895039894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  49%|████████████████████▏                    |  ETA: 0:03:21\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 490000: 24400.98085444695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  50%|████████████████████▍                    |  ETA: 0:03:16\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 500000 saved to ./RL_models/vtol_ppo_2_500000.bson\n",
      "test reward at step 500000: 21926.8284949202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  51%|████████████████████▊                    |  ETA: 0:03:13\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 510000: 31023.81670191517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  52%|█████████████████████▍                   |  ETA: 0:03:08\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 520000: 40584.23639826264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  53%|█████████████████████▋                   |  ETA: 0:03:04\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 530000: 25181.78221849841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  53%|█████████████████████▉                   |  ETA: 0:03:01\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 540000: 51673.34555053218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  55%|██████████████████████▌                  |  ETA: 0:02:55\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 550000: 42337.661387177715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  56%|██████████████████████▊                  |  ETA: 0:02:53\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 560000: 67750.58288644995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  56%|███████████████████████▏                 |  ETA: 0:02:52\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 570000: 48520.79927252033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  58%|███████████████████████▋                 |  ETA: 0:02:45\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 580000: 58174.6218151397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  58%|████████████████████████                 |  ETA: 0:02:43\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 590000: 77897.99754351944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  59%|████████████████████████▍                |  ETA: 0:02:40\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 600000 saved to ./RL_models/vtol_ppo_2_600000.bson\n",
      "test reward at step 600000: 154246.4558890657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  61%|█████████████████████████                |  ETA: 0:02:35\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 610000: 96595.06197629748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  62%|█████████████████████████▎               |  ETA: 0:02:33\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 620000: 86091.9828854186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  63%|█████████████████████████▉               |  ETA: 0:02:30\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 630000: 92502.04698605643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  64%|██████████████████████████▏              |  ETA: 0:02:26\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 640000: 112859.44147371271\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  65%|██████████████████████████▌              |  ETA: 0:02:24\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 650000: 91821.70404188079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  66%|███████████████████████████              |  ETA: 0:02:18\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 660000: 113715.86050433118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  67%|███████████████████████████▎             |  ETA: 0:02:16\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 670000: 130997.10200843193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  67%|███████████████████████████▋             |  ETA: 0:02:14\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 680000: 163327.82479413928\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  69%|████████████████████████████▎            |  ETA: 0:02:08\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 690000: 57520.320157535396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  70%|████████████████████████████▌            |  ETA: 0:02:06\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 700000 saved to ./RL_models/vtol_ppo_2_700000.bson\n",
      "test reward at step 700000: 37940.41240120534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  70%|████████████████████████████▉            |  ETA: 0:02:03\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 710000: 187640.88261707948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  72%|█████████████████████████████▌           |  ETA: 0:01:57\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 720000: 217584.8465656009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  73%|█████████████████████████████▊           |  ETA: 0:01:55\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 730000: 278931.44674931985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  73%|██████████████████████████████▏          |  ETA: 0:01:53\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 740000: 298500.12551030767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  75%|██████████████████████████████▊          |  ETA: 0:01:47\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 750000: 539855.6401860852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  76%|███████████████████████████████          |  ETA: 0:01:46\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 760000: 79502.59391161131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  76%|███████████████████████████████▍         |  ETA: 0:01:43\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 770000: 219690.90884988612\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  78%|████████████████████████████████         |  ETA: 0:01:37\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 780000: 560528.187996154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  79%|████████████████████████████████▎        |  ETA: 0:01:36\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 790000: 529798.4745197479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  79%|████████████████████████████████▌        |  ETA: 0:01:34\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 800000 saved to ./RL_models/vtol_ppo_2_800000.bson\n",
      "test reward at step 800000: 347012.5090271418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  81%|█████████████████████████████████▏       |  ETA: 0:01:27\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 810000: 478462.1722484009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  82%|█████████████████████████████████▌       |  ETA: 0:01:25\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 820000: 119476.08393930033\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  82%|█████████████████████████████████▊       |  ETA: 0:01:22\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 830000: 441569.98038484534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  84%|██████████████████████████████████▍      |  ETA: 0:01:16\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 840000: 207815.049588758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  85%|██████████████████████████████████▊      |  ETA: 0:01:13\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 850000: 692799.5022442362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  85%|███████████████████████████████████      |  ETA: 0:01:10\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 860000: 922631.892110704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  87%|███████████████████████████████████▋     |  ETA: 0:01:04\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 870000: 163720.66205169013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  88%|███████████████████████████████████▉     |  ETA: 0:01:01\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 880000: 315.8492141528139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  88%|████████████████████████████████████▎    |  ETA: 0:00:57\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 890000: 969480.457131844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  90%|████████████████████████████████████▉    |  ETA: 0:00:50\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 900000 saved to ./RL_models/vtol_ppo_2_900000.bson\n",
      "test reward at step 900000: 759416.2937521525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  91%|█████████████████████████████████████▏   |  ETA: 0:00:47\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 910000: 750744.5068174687\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  92%|█████████████████████████████████████▊   |  ETA: 0:00:42\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 920000: 898012.2883187303\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  93%|██████████████████████████████████████   |  ETA: 0:00:38\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 930000: 713082.7388620012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  93%|██████████████████████████████████████▍  |  ETA: 0:00:34\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 940000: 1.119983812523094e6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  95%|██████████████████████████████████████▉  |  ETA: 0:00:27\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 950000: 1.5112882181439463e6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  96%|███████████████████████████████████████▎ |  ETA: 0:00:23\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 960000: 1.0849653977598925e6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  97%|███████████████████████████████████████▊ |  ETA: 0:00:16\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 970000: 1.2737276580355105e6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  98%|████████████████████████████████████████ |  ETA: 0:00:13\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 980000: 486103.24072085787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress:  99%|████████████████████████████████████████▋|  ETA: 0:00:06\u001B[39m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test reward at step 990000: 254455.7542446831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32mProgress: 100%|█████████████████████████████████████████| Time: 0:09:15\u001B[39m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parameters at step 1000000 saved to ./RL_models/vtol_ppo_2_1000000.bson\n",
      "test reward at step 1000000: 42977.263683668876\n"
     ]
    }
   ],
   "source": [
    "#run actual training\n",
    "#here: connect tensorboard\n",
    "run(\n",
    "           agent,\n",
    "           env,\n",
    "           StopAfterStep(1_000_000),\n",
    "           ComposedHook(\n",
    "                DoEveryNStep(saveModel, n=100_000), \n",
    "                DoEveryNStep(validate_policy, n=10_000)),\n",
    "       )"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6643cd20",
   "metadata": {},
   "source": [
    "close_visualization(); # closes the MeshCat visualization"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.8.0",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
